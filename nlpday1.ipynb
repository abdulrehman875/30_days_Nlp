{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **TASK2**"
      ],
      "metadata": {
        "id": "D9zPzd6KXWpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZTION**"
      ],
      "metadata": {
        "id": "f4CTU3DYXdqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#  text\n",
        "sample_text = \"Tokenization is a crucial preprocessing step in natural language processing tasks.\"\n",
        "\n",
        "# Tokenize the different text\n",
        "tokenized_text = word_tokenize(sample_text)\n",
        "print(tokenized_text)\n"
      ],
      "metadata": {
        "id": "TQFtQQkAXVg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eeb71c1-16a1-46ce-adb0-61ae2b796e07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'a', 'crucial', 'preprocessing', 'step', 'in', 'natural', 'language', 'processing', 'tasks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SPACY**"
      ],
      "metadata": {
        "id": "l2ecCZqWZFIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Different text\n",
        "example_text = \"Text tokenization plays a vital role in NLP for understanding textual data.\"\n",
        "\n",
        "# Process the different text\n",
        "doc = nlp(example_text)\n",
        "\n",
        "# Extract tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4JcmZSzZIo3",
        "outputId": "4aa14258-918e-4b5d-df99-ab3a66b6b9dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Text', 'tokenization', 'plays', 'a', 'vital', 'role', 'in', 'NLP', 'for', 'understanding', 'textual', 'data', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TASK3**"
      ],
      "metadata": {
        "id": "zuDprms6ZTem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STOPWORDS**"
      ],
      "metadata": {
        "id": "Tiq0ucxGZXPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Stopwords are common words that often occur in text but typically do not carry significant meaning.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Get English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original tokens:\", tokens)\n",
        "print(\"Filtered tokens without stopwords:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZRd5OaiZdID",
        "outputId": "4da92183-6515-4070-93d0-1f0726acbd3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['Stopwords', 'are', 'common', 'words', 'that', 'often', 'occur', 'in', 'text', 'but', 'typically', 'do', 'not', 'carry', 'significant', 'meaning', '.']\n",
            "Filtered tokens without stopwords: ['Stopwords', 'common', 'words', 'often', 'occur', 'text', 'typically', 'carry', 'significant', 'meaning', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK4**"
      ],
      "metadata": {
        "id": "dIS796FwZvU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"Stemming and lemmatization are important techniques for preprocessing textual data.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "# Initialize Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_tokens = [porter_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(\"Lemmatized tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA18BPWeZ2Pv",
        "outputId": "d6b0445d-47da-4196-b563-f0269b00c235"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens: ['stem', 'and', 'lemmat', 'are', 'import', 'techniqu', 'for', 'preprocess', 'textual', 'data', '.']\n",
            "Lemmatized tokens: ['Stemming', 'and', 'lemmatization', 'are', 'important', 'technique', 'for', 'preprocessing', 'textual', 'data', '.']\n"
          ]
        }
      ]
    }
  ]
}