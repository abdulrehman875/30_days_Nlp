{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Sample text data\n",
        "text = \"\"\"To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them.\"\"\"\n",
        "\n",
        "# Preprocess the text\n",
        "text = text.lower().replace('\\n', ' ').replace(':', '').replace(',', '').replace(\"'\", '').replace('.', '')\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequences\n",
        "input_sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# Set sequence length\n",
        "sequence_length = 5\n",
        "\n",
        "# Create input-output pairs\n",
        "sequences = []\n",
        "for i in range(sequence_length, len(input_sequence)):\n",
        "    seq = input_sequence[i-sequence_length:i]\n",
        "    label = input_sequence[i]\n",
        "    sequences.append((seq, label))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array([seq for seq, label in sequences])\n",
        "y = np.array([label for seq, label in sequences])\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Print shapes to debug\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'y shape: {y.shape}')\n",
        "print(f'Example X: {X[0]}')\n",
        "print(f'Example y: {y[0]}')\n",
        "\n",
        "# Build the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_words, output_dim=64, input_length=sequence_length))  # Correct input_length to sequence_length\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqr4NvxcEvg0",
        "outputId": "21c2d9ab-4739-4366-9c44-165f58ac53de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (34, 5)\n",
            "y shape: (34, 31)\n",
            "Example X: [1 3 4 7 1]\n",
            "Example y: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 2s 11ms/step - loss: 3.4345 - accuracy: 0.0588\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.4287 - accuracy: 0.1176\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.4244 - accuracy: 0.0882\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3.4206 - accuracy: 0.0882\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.4170 - accuracy: 0.1471\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.4133 - accuracy: 0.2059\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.4096 - accuracy: 0.2353\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.4058 - accuracy: 0.2647\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.4017 - accuracy: 0.2647\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.3975 - accuracy: 0.2353\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3931 - accuracy: 0.2353\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3887 - accuracy: 0.2059\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.3834 - accuracy: 0.2059\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3781 - accuracy: 0.1765\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3720 - accuracy: 0.1765\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.3654 - accuracy: 0.1765\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3581 - accuracy: 0.2059\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3502 - accuracy: 0.2059\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3413 - accuracy: 0.2059\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.3310 - accuracy: 0.2059\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.3203 - accuracy: 0.2059\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.3081 - accuracy: 0.1765\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.2949 - accuracy: 0.1765\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.2804 - accuracy: 0.1765\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3.2640 - accuracy: 0.1765\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.2452 - accuracy: 0.1765\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 3.2231 - accuracy: 0.1765\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3.1964 - accuracy: 0.1765\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3.1658 - accuracy: 0.1765\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.1322 - accuracy: 0.1765\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.0944 - accuracy: 0.1765\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3.0550 - accuracy: 0.1765\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.0146 - accuracy: 0.1765\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.9732 - accuracy: 0.1765\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.9252 - accuracy: 0.2059\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.8768 - accuracy: 0.2059\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.8253 - accuracy: 0.1765\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.7765 - accuracy: 0.1765\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2.7322 - accuracy: 0.2059\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.6879 - accuracy: 0.1765\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.6428 - accuracy: 0.2059\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.5993 - accuracy: 0.2059\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.5565 - accuracy: 0.2353\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.5147 - accuracy: 0.2941\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2.4761 - accuracy: 0.2647\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.4384 - accuracy: 0.2941\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.3992 - accuracy: 0.3235\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.3625 - accuracy: 0.3235\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.3228 - accuracy: 0.2941\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.2873 - accuracy: 0.2941\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.2563 - accuracy: 0.2941\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.2224 - accuracy: 0.2941\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.1836 - accuracy: 0.2941\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.1563 - accuracy: 0.3235\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.1344 - accuracy: 0.3235\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.1077 - accuracy: 0.3529\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2.0687 - accuracy: 0.3529\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.0224 - accuracy: 0.4118\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.9801 - accuracy: 0.4412\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.9485 - accuracy: 0.4412\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.9176 - accuracy: 0.4412\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.8913 - accuracy: 0.4412\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.8569 - accuracy: 0.4412\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.8141 - accuracy: 0.4412\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.7780 - accuracy: 0.4412\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.7426 - accuracy: 0.4706\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.7083 - accuracy: 0.5000\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.6773 - accuracy: 0.4706\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.6452 - accuracy: 0.4706\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.6036 - accuracy: 0.4706\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.5649 - accuracy: 0.5000\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.5275 - accuracy: 0.5294\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.4938 - accuracy: 0.5588\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.4622 - accuracy: 0.5588\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.4298 - accuracy: 0.5000\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.3982 - accuracy: 0.5294\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3650 - accuracy: 0.5588\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3352 - accuracy: 0.5882\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.3085 - accuracy: 0.6176\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2795 - accuracy: 0.6176\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.2485 - accuracy: 0.6765\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.2161 - accuracy: 0.7059\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.1847 - accuracy: 0.7353\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.1601 - accuracy: 0.7647\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.1360 - accuracy: 0.7353\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.1177 - accuracy: 0.6765\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.0982 - accuracy: 0.7353\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.0740 - accuracy: 0.6765\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.0455 - accuracy: 0.7353\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1.0260 - accuracy: 0.7059\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.0081 - accuracy: 0.7059\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.9824 - accuracy: 0.7941\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.9627 - accuracy: 0.7941\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.9449 - accuracy: 0.8235\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.9282 - accuracy: 0.8529\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.9112 - accuracy: 0.8824\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.8936 - accuracy: 0.8824\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.8758 - accuracy: 0.8235\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.8609 - accuracy: 0.8235\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8483 - accuracy: 0.8235\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792e852dec50>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_index = np.argmax(predicted, axis=1)[0]\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"and\"\n",
        "next_words = 10\n",
        "generated_text = generate_text(seed_text, next_words, model, sequence_length)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq3lLeJeI7zA",
        "outputId": "a17d1eb9-5955-4cad-dab0-98782d26943e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and arrows arrows or or or to take against against a\n"
          ]
        }
      ]
    }
  ]
}