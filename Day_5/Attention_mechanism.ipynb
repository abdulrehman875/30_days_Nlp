{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DmqQdP8hffC2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "english_sentences = ['start he is a student end', 'start world is temporary end', 'start Many people eat cereal for breakfast end', 'start I am good end', 'start Ted goes to the gym and exercises three times a week end']\n",
        "russian_sentences = ['start он студент end', 'start мир временен end', 'start Многие люди едят хлопья на завтрак end', 'start У меня все хорошо end', 'start Тед ходит в спортзал и тренируется три раза в неделю. end']\n",
        "\n",
        "source_tokenizer = Tokenizer()\n",
        "source_tokenizer.fit_on_texts(english_sentences)\n",
        "source_sequences = source_tokenizer.texts_to_sequences(english_sentences)\n",
        "input_texts = pad_sequences(source_sequences, padding='post')\n",
        "\n",
        "target_tokenizer = Tokenizer()\n",
        "target_tokenizer.fit_on_texts(russian_sentences)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(russian_sentences)\n",
        "target_texts = pad_sequences(target_sequences, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate\n",
        "from tensorflow.keras.layers import AdditiveAttention as Attention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "latent_dim = 512\n",
        "num_encoder_tokens = len(source_tokenizer.word_index) + 1\n",
        "num_decoder_tokens = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(num_encoder_tokens, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(num_decoder_tokens, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "# Attention Layer\n",
        "attention = Attention()\n",
        "attention_output = attention([decoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenating attention output and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1)([decoder_outputs, attention_output])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)"
      ],
      "metadata": {
        "id": "j7YHVB99sD1r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "w3kg39EGsRqd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "decoder_target_data = to_categorical(target_texts, num_decoder_tokens)\n",
        "model.fit([input_texts, target_texts], decoder_target_data, batch_size=64, epochs=50, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqlefWKpsU_g",
        "outputId": "d8c5963a-3625-4ae9-cb54-acb4c0b6cdc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 7s 7s/step - loss: 3.2499 - accuracy: 0.0208 - val_loss: 3.2603 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 1s 574ms/step - loss: 2.9783 - accuracy: 0.5417 - val_loss: 3.2678 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 2.5398 - accuracy: 0.5417 - val_loss: 3.4240 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 2.0151 - accuracy: 0.5417 - val_loss: 3.2771 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 2.4741 - accuracy: 0.5833 - val_loss: 3.3109 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8097 - accuracy: 0.5417 - val_loss: 3.7752 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.6881 - accuracy: 0.5417 - val_loss: 3.3319 - val_accuracy: 0.0833\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 1.7509 - accuracy: 0.6250 - val_loss: 3.6399 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 1.5095 - accuracy: 0.6042 - val_loss: 3.4364 - val_accuracy: 0.0833\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 1.4297 - accuracy: 0.6250 - val_loss: 3.7445 - val_accuracy: 0.0833\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 1.3587 - accuracy: 0.6250 - val_loss: 3.6286 - val_accuracy: 0.0833\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 274ms/step - loss: 1.3147 - accuracy: 0.6667 - val_loss: 4.0849 - val_accuracy: 0.0833\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 1.2807 - accuracy: 0.6250 - val_loss: 3.7744 - val_accuracy: 0.0833\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 279ms/step - loss: 1.2715 - accuracy: 0.6250 - val_loss: 4.6786 - val_accuracy: 0.0833\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 1.3309 - accuracy: 0.6250 - val_loss: 3.7016 - val_accuracy: 0.0833\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 1.3089 - accuracy: 0.6458 - val_loss: 4.5586 - val_accuracy: 0.0833\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 1.2622 - accuracy: 0.6250 - val_loss: 3.8474 - val_accuracy: 0.0833\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 1.1578 - accuracy: 0.6458 - val_loss: 4.7318 - val_accuracy: 0.0833\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 1.0943 - accuracy: 0.6458 - val_loss: 4.4446 - val_accuracy: 0.0833\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 1.0556 - accuracy: 0.6250 - val_loss: 5.4173 - val_accuracy: 0.0833\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 1.0469 - accuracy: 0.6250 - val_loss: 4.8184 - val_accuracy: 0.0833\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 1.0882 - accuracy: 0.6458 - val_loss: 4.8745 - val_accuracy: 0.0833\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 1.2028 - accuracy: 0.6250 - val_loss: 4.0699 - val_accuracy: 0.0833\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 1.0547 - accuracy: 0.6458 - val_loss: 5.1430 - val_accuracy: 0.0833\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 1.0005 - accuracy: 0.6458 - val_loss: 4.6169 - val_accuracy: 0.0833\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.9370 - accuracy: 0.6875 - val_loss: 5.3594 - val_accuracy: 0.0833\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.9008 - accuracy: 0.6875 - val_loss: 5.4500 - val_accuracy: 0.0833\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.8815 - accuracy: 0.7083 - val_loss: 5.9296 - val_accuracy: 0.0833\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.9526 - accuracy: 0.6667 - val_loss: 5.3524 - val_accuracy: 0.0833\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.9622 - accuracy: 0.6875 - val_loss: 4.9267 - val_accuracy: 0.0833\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 1.0438 - accuracy: 0.6250 - val_loss: 4.2900 - val_accuracy: 0.0833\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.8462 - accuracy: 0.7083 - val_loss: 5.6795 - val_accuracy: 0.0833\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.7769 - accuracy: 0.7083 - val_loss: 5.3888 - val_accuracy: 0.0833\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 0.7396 - accuracy: 0.7708 - val_loss: 6.5550 - val_accuracy: 0.0833\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 290ms/step - loss: 0.7251 - accuracy: 0.7292 - val_loss: 6.0235 - val_accuracy: 0.0833\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.7459 - accuracy: 0.6875 - val_loss: 5.9560 - val_accuracy: 0.0833\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 276ms/step - loss: 0.8386 - accuracy: 0.6667 - val_loss: 5.2051 - val_accuracy: 0.0833\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.8108 - accuracy: 0.6875 - val_loss: 4.9142 - val_accuracy: 0.0833\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.8469 - accuracy: 0.6458 - val_loss: 6.0591 - val_accuracy: 0.0833\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 287ms/step - loss: 0.6822 - accuracy: 0.7917 - val_loss: 5.5578 - val_accuracy: 0.0833\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 443ms/step - loss: 0.7084 - accuracy: 0.7917 - val_loss: 6.4848 - val_accuracy: 0.0833\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 0.6648 - accuracy: 0.8125 - val_loss: 6.1005 - val_accuracy: 0.0833\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 386ms/step - loss: 0.7225 - accuracy: 0.7500 - val_loss: 6.0779 - val_accuracy: 0.0833\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 0.6290 - accuracy: 0.7917 - val_loss: 6.1438 - val_accuracy: 0.0833\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 0.6372 - accuracy: 0.7708 - val_loss: 6.0128 - val_accuracy: 0.0833\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 0.6129 - accuracy: 0.7917 - val_loss: 5.8305 - val_accuracy: 0.0833\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 424ms/step - loss: 0.6368 - accuracy: 0.7500 - val_loss: 6.3992 - val_accuracy: 0.0833\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 0.6020 - accuracy: 0.7708 - val_loss: 5.4222 - val_accuracy: 0.0833\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.6456 - accuracy: 0.7292 - val_loss: 6.3423 - val_accuracy: 0.0833\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.5509 - accuracy: 0.8125 - val_loss: 5.9452 - val_accuracy: 0.0833\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e03bd0e7b50>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Uf5HqcSponj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e85c57-be87-4cbc-b64d-8520310d8664"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 256)            7168      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 256)            6656      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 512),                1574912   ['embedding[0][0]']           \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 512),          1574912   ['embedding_1[0][0]',         \n",
            "                              (None, 512),                           'lstm[0][1]',                \n",
            "                              (None, 512)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " additive_attention (Additi  (None, None, 512)            512       ['lstm_1[0][0]',              \n",
            " veAttention)                                                        'lstm[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, None, 1024)           0         ['lstm_1[0][0]',              \n",
            "                                                                     'additive_attention[0][0]']  \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 26)             26650     ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3190810 (12.17 MB)\n",
            "Trainable params: 3190810 (12.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}